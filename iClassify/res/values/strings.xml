<?xml version="1.0" encoding="utf-8"?>
<resources>

    <string name="app_name">iClassify</string>
    <string name="hello_world">Hello world!</string>
    <string name="action_settings">Settings</string>
    <string name="empty" />
    <string name="md__drawerOpenIndicatorDesc">Close drawer</string>
    <string name="md__drawerClosedIndicatorDesc">Open drawer</string>
    <string name="ok">Ok</string>
    <string name="cancel">Cancel</string>
    <string name="imgdesc">imgdesc</string>
    <string name="goback">Go Back</string>

    <string-array name="titles">
        <item>Discussion</item>
        <item>Videos</item>
        <item>Simulation</item>
        <item>Problems</item>
        <item>Exit</item>
    </string-array>

    <array name="icons">
        <item>@drawable/drawer_discuss</item>
        <item>@drawable/drawer_videos</item>
        <item>@drawable/navdrawer_simul</item>
        <item>@drawable/navdrawer_assess</item>
        <item>@drawable/navdrawer_exit</item>
    </array>

    <string name="action_left">Decision Tree</string>
    <string name="action_mid">K-Nearest Neighbor</string>
    <string name="action_right">Native Bayesian</string>

    <!-- Video Description -->

    <string name="introvideodesc">
<![CDATA[
 			Classification consists of predicting a certain outcome based on a given input. In order to predict the outcome, the algorithm processes a training set containing a set of attributes and the respective outcome, usually called goal or prediction attribute. The algorithm tries to discover relationships between the attributes that would make it possible to predict the outcome. Next the algorithm is given a data set not seen before, called prediction set, which contains the same set of attributes, except for the prediction attribute – not yet known. The algorithm analyses the input and produces a prediction.
 			]]>
    </string>
    <string name="knnvideodesc">
<![CDATA[
 			This is how the K-Nearest Neighbour algorithm works. Firstly we have a variable "K". With this, you assign "K" a value from 1 to the total number of examples. The best value to chose for K is a low odd number. With this value of "K", we look at the k closest points on the graph from the location of the new example point. The next part is to assign the new point a class, the class type will be determined from the majority of the closest points.\n\n
			
			Algorithm in short:\n
			1. Assign K a value -- preferably a small odd number.\n
			2. Find the closest number of K points.\n
			3. Assign the new point from the majority of classes.\n
 			]]>
    </string>
    <string name="naivevideodesc">
<![CDATA[
 			The Naive Bayesian classifier is based on Bayes theorem with independence assumptions between predictors. A Naive Bayesian model is easy to build, with no complicated iterative parameter estimation which makes it particularly useful for very large datasets. Despite its simplicity, the Naive Bayesian classifier often does surprisingly well and is widely used because it often outperforms more sophisticated classification methods.

 			]]>
    </string>
    <string name="treevideodesc">
<![CDATA[
 			The best way to see a Decision Tree is with an example. A decision tree is like a flowchart diagram where you can read off your decision rules. We can see why such diagrams are called trees, because, while they are admittedly upside down, they start from a root and have branches leading to leaves. The leaves are always decisions, and a particular decision might be at the end of multiple branches.

 			]]>
    </string>
    <string name="pracprob1">
<![CDATA[
 			Suppose we want the ID3 algorithm to decide whether the weather is amenable to playing baseball. Over the course of 2 weeks, data is collected to help ID3 build a decision tree . 
 			]]>
    </string>
    <string name="weathattrib">
<![CDATA[
 			 The weather attributes are outlook, temperature, humidity, and wind speed.\n
 			]]>
    </string>
    <string name="dataset1">
<![CDATA[
 			We need to find which attribute will be the root node in our decision tree. The gain is calculated for all four attributes: \n
 			
 			]]>
    </string>
    <string name="example2desc">
<![CDATA[
        Suppose S is a set of 14 examples in which one of the attributes is wind speed. The values of Wind can be Weak or Strong. The classification of these 14 examples are 9 YES and 5 NO. For attribute Wind, suppose there are 8 occurrences of Wind = Weak and 6 occurrences of Wind = Strong. For Wind = Weak, 6 of the examples are YES and 2 are NO. For Wind = Strong, 3 are YES and 3 are NO. Therefore
        ]]>
    </string>
    <string name="gainwind">
<![CDATA[
        Suppose S is a set of 14 examples in which one of the attributes is wind speed. The values of Wind can be Weak or Strong. The classification of these 14 examples are 9 YES and 5 NO. For attribute Wind, suppose there are 8 occurrences of Wind = Weak and 6 occurrences of Wind = Strong. For Wind = Weak, 6 of the examples are YES and 2 are NO. For Wind = Strong, 3 are YES and 3 are NO. Therefore
        ]]>
    </string>
    <string name="gaintempe">
<![CDATA[
        Suppose S is a set of 14 examples in which one of the attributes is temperature. The values of Temperature can be Hot, mild and cold. The classification of these 14 examples are 9 YES and 5 NO. For attribute temperature, suppose there are 4 occurrences of Temperature = Hot and 6 occurrences of Temperature = Mild and 4 occurrences of Temperature = Cool. For Wind = Hot, 2 of the examples are YES and 2 are NO. For Wind = Mild, 4 are YES and 2 are NO. Therefore
        ]]>
    </string>
    <string name="gainhumidity">
<![CDATA[
        Suppose S is a set of 14 examples in which one of the attributes is wind speed. The values of Wind can be Weak or Strong. The classification of these 14 examples are 9 YES and 5 NO. For attribute Wind, suppose there are 8 occurrences of Wind = Weak and 6 occurrences of Wind = Strong. For Wind = Weak, 6 of the examples are YES and 2 are NO. For Wind = Strong, 3 are YES and 3 are NO. Therefore
        ]]>
    </string>

    <!-- Calculation -->

    <string name="o1">
<![CDATA[
       =0.940-(5/14)*0.971 -(4/14)*0.0 – (5/14)*0.0971
	   =0.247
        ]]>
    </string>
    <string name="h1">
<![CDATA[
        =0.940-(7/14)*0.985 – (7/14)*0.592
		=0.151
        ]]>
    </string>
    <string name="w1">
<![CDATA[
        = 0.940 - (8/14)*0.811 - (6/14)*1.00\n
        = 0.048
        ]]>
    </string>
    <string name="wweak">
<![CDATA[
        Entropy(Sweak) = - (6/8)*log2(6/8) - (2/8)*log2(2/8) = 0.81
        ]]>
    </string>
    <string name="wstrong">
<![CDATA[
        Entropy(Sstrong) = - (3/6)*log2(3/6) - (3/6)*log2(3/6) = 1.00
        ]]>
    </string>
    <string name="correctdecision1">
<![CDATA[
       Since Outlook has three possible values, the root node has three branches (sunny, overcast, rain)."
        ]]>
    </string>
    <string name="correctdecision2">
<![CDATA[
       Since we`ve used Outlook at the root, we only decide on the remaining three attributes: Humidity, Temperature, or Wind.
]]>
    </string>
    <string name="restext">
<![CDATA[
    The final decision = tree\n
	The decision tree can also be expressed in rule format:
]]>
    </string>
    <string name="outlookoccurencesrain">
<![CDATA[
        Rain Outlook\n
		For Outlook = Rain, 3 are YES and 2 are NO. 
		]]>
    </string>
    <string name="outlookoccurencessunny">
<![CDATA[
        Sunny Outlook\n
		For Outlook = Sunny, 2 are YES and 3 are NO. 
		]]>
    </string>
    <string name="humidityoccurenceshigh">
<![CDATA[
        High Humidity\n
		For Humidty = High, 3 are YES and 4 are NO. 
	]]>
    </string>
    <string name="humidityoccurencesnormal">
<![CDATA[
        Low Humidity\n
		For Humidity = Normal, 6 are YES and 1 are NO. 
	]]>
    </string>
    <string name="windoccurencesweak">
<![CDATA[
      Weak Wind\n
      The classification of these 14 examples are 9 YES and 5 NO. For attribute Wind, suppose there are 8 occurrences of Wind = Weak.\n
	  For Wind = Weak, 6 of the examples are YES and 2 are NO.
	]]>
    </string>
    <string name="windoccurencesstrong">
<![CDATA[
        Strong Wind\n
        The classification of these 14 examples are 9 YES and 5 NO. For attribute Wind, suppose there are 6 occurrences of Wind = Strong.\n
		For Wind = Strong, 3 are YES and 3 are NO. 
	]]>
    </string>
    <string name="humidityresult">
<![CDATA[
       Humidity has the highest gain; therefore, it is used as the decision node. This process goes on until all data is classified perfectly or we run out of attributes.
	]]>
    </string>
    <string name="idrresultdesc">
<![CDATA[
       ID3 has been incorporated in a number of commercial rule-induction packages. Some specific applications include medical diagnosis, credit risk assessment of loan applications, equipment malfunctions by their cause, classification of soybean diseases, and web search classification.
	]]>
    </string>
    <string name="naiveintrotext">
<![CDATA[
       Hi Students, blah blah blah blah blah blah blah blah blah blah blah blah blah blah blahblah blah blah blah blahblah blah blah blah blahblah blah blah blah blah
	]]>
    </string>
    <string name="naivebayes1">
<![CDATA[
       Lets say we have a table that decided if we should play tennis under certain circumstances. These could be theoutlook of the weather; the temperature; the humidity and the strength of the wind:
	]]>
    </string>
    <string name="naivebayes2">
<![CDATA[
      So here we have 4 attributes. What we need to do is to create “look-up tables” for each of these attributes, and write in the probability that a game of tennis will be played based on this attribute. In these tables we have to note that there are 5 cases of not being able to play a game, and 9 cases of being able to play a game. (tap the icon below to view the specific dataset)
	]]>
    </string>
    <string name="naivetesting1">
<![CDATA[
  	Now, we are in the testing phase. For this, say we were given a new instance, and we want to know if we can play a game or not, then we need to lookup the results from the tables above. So, this new instance is:\n
	X = (Outlook=Sunny, Temperature=Cool, Humidity=High, Wind=Strong)
	]]>
    </string>
    <string name="naiveplaygame">
<![CDATA[
  	 Firstly we look at the probability that we can play the game, so we use the lookup tables to get:
	\n\n
	P(Outlook=Sunny | Play=Yes) = 2/9\n
	P(Temperature=Cool | Play=Yes) = 3/9 \n
	P(Humidity=High | Play=Yes) = 3/9 \n
	P(Wind=Strong | Play=Yes) = 3/9 \n
	P(Play=Yes) = 9/14 \n
	]]>
    </string>
    <string name="naivedontplaygame">
<![CDATA[
  	Next we consider the fact that we cannot play a game:\n\n
	P(Outlook=Sunny | Play=No) = 3/5\n
	P(Temperature=Cool | Play=No) = 1/5\n
	P(Humidity=High | Play=No) = 4/5\n
	P(Wind=Strong | Play=No) = 3/5\n
	P(Play=No) = 5/14\n
	]]>
    </string>
    <string name="calculatenaive1">
<![CDATA[
  	Then, using those results, you have to multiple the whole lot together. So you multiple all the probabilities for Play=Yes such as:\n
	P(X|Play=Yes)P(Play=Yes) = (2/9) * (3/9) * (3/9) * (3/9) * (9/14) = 0.0053\n
	And this gives us a value that represents ‘P(X|C)P(C)’, or in this case ‘P(X|Play=Yes)P(Play=Yes)’.
	
	]]>
    </string>
    <string name="calculatenaive2">
<![CDATA[
  	We also have to do the same thing for Play=No:\n\n
	P(X|Play=No)P(Play=No) = (3/5) * (1/5) * (4/5) * (3/5) * (5/14) = 0.0206
	]]>
    </string>
    <string name="calculatenaive3">
<![CDATA[
  	Finally, we have to divide both results by the evidence, or ‘P(X)’. The evidence for both equations
	is the same, and we can find the values we need within the ‘Total’ columns of the look-up tables. Therefore:
	]]>
    </string>
    <string name="calculatenaive4">
<![CDATA[
  	P(X) = P(Outlook=Sunny) * P(Temperature=Cool) * P(Humidity=High) * P(Wind=Strong)\n
	P(X) = (5/14) * (4/14) * (7/14) * (6/14)\n
	P(X) = 0.02186\n
	]]>
    </string>
    <string name="calculatenaive5">
<![CDATA[
  	Then, dividing the results by this value:\n
	P(Play=Yes | X) = 0.0053/0.02186 = 0.2424\n
	P(Play=No | X) = 0.0206/0.02186 = 0.9421\n
	]]>
    </string>
    <string name="calculatenaive6">
<![CDATA[
  	So, given the probabilities, can we play a game or not? To do this, we look at both probabilities and see which once has the highest value, and that is our answer. Therefore:\n
	P(Play=Yes | X) = 0.2424\n
	P(Play=No | X) = 0.9421\n
	]]>
    </string>
    <string name="calculatenaive7">
<![CDATA[
  	Since 0.9421 is greater than 0.2424 then the answer is ‘no’, we cannot play a game of tennis today.
	]]>
    </string>

    <!-- Knn -->
    <string name="knnsimulprocess1">
<![CDATA[
  	1. Determine parameter K = number of nearest neighbors
	]]>
    </string>
    <string name="knnsimulprocess2">
<![CDATA[
  	2. Calculate the distance between the query-instance and all the training samples
	]]>
    </string>
    <string name="knnsimulprocess3">
<![CDATA[
  	3.Sort the distance and determine nearest neighbors based on the K-th minimum distance
	]]>
    </string>
    <string name="knnsimulprocess4">
<![CDATA[
  	4.Gather the category Y of the nearest neighbors
	]]>
    </string>
    <string name="knnsimulprocess5">
<![CDATA[
  	5.Use simple majority of the category of nearest neighbors as the prediction value of the query instance
	]]>
    </string>
    <string name="knnsimulexample1">
<![CDATA[
  	We have data from the questionnaires survey (to ask people opinion) and objective testing with two attributes (acid durability and strength) to classify whether a special paper tissue is good or not.\n 
  	Here is four training samples
	]]>
    </string>
    <string name="knnsimulexample2">
<![CDATA[
  	Now the factory produces a new paper tissue that pass laboratory test with X1 = 3 and X2 = 7.
	]]>
    </string>
    <string name="knnsimulexample3">
<![CDATA[
  	Without another expensive survey, can we guess what the classification of this new tissue is?
	]]>
    </string>
    <string name="knnsimulexample4">
<![CDATA[
  	Step 1. Determine parameter K = number of nearest neighbors
	]]>
    </string>
    <string name="knnsimulexample5">
<![CDATA[
  	Step 2. Calculate the Distance between query-instance and all training samples\n
  	Coordinate of query instance is (3, 7), instead of calculating the distance we compute square distance which is faster to calculate (without square root)
	]]>
    </string>
    <string name="SimpsonWeight">
<![CDATA[
  	Of the 3 features we had, Weight was best. But while people who weigh over 160 are perfectly classified (as males), the under 160 people are not perfectly classified… So we simply recurse!\n\n
	This time we find that we can split on Hair length, and we are done!

	]]>
    </string>
    <string name="SimpsonMaleClass">
<![CDATA[
  	We need to keep the data around, just the test conditions.

	]]>
    </string>
    <string name="isweight160">
<![CDATA[
  	is Weight <= 160?
	]]>
    </string>
    <string name="ishairlength">
<![CDATA[
  	is Hair length <= 2?
	]]>
    </string>
    <string name="lookuptextpatient">
<![CDATA[
  	So here we have 4 attributes. What we need to do is to create “look-up tables” for each of these attributes,\n
  	 and write in the probability that the patient has flu based on this attribute. In these tables we have to 
  	 note that there are 5 cases of a patient having flu,\n
	 and 3 cases of a patient not having a flu.
	]]>
    </string>
    <string name="patienttesting">
<![CDATA[
  	There is a new patient, and we want to know if the patient has flu or not, based on the patient’s \n
  	symptoms and the previous cases.\n\n\n
  	
  	So, this new instance is:
  	
	
	]]>
    </string>
    <string name="patientmultiply">
<![CDATA[
  	Multiply all the probabilities for Flu=Yes such as:\n
	P(X|Flu=Yes)P(Flu=Yes)\n\n
	(3/5) * (1/5) * (2/5) * (1/5) * (5/8) = 
	
	]]>
    </string>
    <string name="textFluNoMultiply">
<![CDATA[
  	P(X|Flu=No)P(Flu=No) \n\n
  	(1/3) * (2/3) * (1/3) * (2/3) * (3/8) =
	]]>
    </string>
    <string name="assessmenttetobjectivesid3">
<![CDATA[
  	DECISION TREE\n
	1. To identify and arrange the correct step by step process in constructing a decision tree.\n
	2. To classify and predict the output using the algorithm.\n
	3.  To practice and test knowledge about the algorithm
	]]>
    </string>
    <string name="problemsimpson">
<![CDATA[
  	What is Comic’s gender? \n
  	Is Comic a male or a female?
	]]>
    </string>
    <string name="simpsonresult">
<![CDATA[
  	After getting Comic’s attribute \n
  	We can now classify Comic as a Male
	]]>
    </string>
    <string name="tablesimpson">
<![CDATA[
  	Based on the table below \n
  	Can you classify Comic’s gender based on his attributes?
	]]>
    </string>
    <string name="problempatient">
<![CDATA[
  	Do Doctor believe that a patient with the following symptoms has a flu?
	]]>
    </string>
    <string name="whatalgoyouwant">
<![CDATA[
  	What algorithm you want \nto solve this problem?
	]]>
    </string>
    
    <string name="jumbleid3question1">
        It characterizes the impurity of an arbitrary 
		collection of examples.</string>
    
    <string name="patientcomputestatement1">
	<![CDATA[
  	Firstly we look at the probability that we can play the game, so we use the lookup tables to get:
	]]>
    </string>
    <string name="patientcomputestatement2">
	<![CDATA[
  	Next we consider the fact that we cannot play a game:
	]]>
    </string>
     <string name="patientcomputestatement3">
	<![CDATA[
  	Then, using those results, you have to multiply the whole lot together. So you multiply all the probabilities for Flu=Yes such as:
	]]>
    </string>
     <string name="patientcomputestatement4">
	<![CDATA[
  	We also have to do the same thing for Flu=No:
	]]>
    </string>
    
    <string name="patientcomputestatement5">
	<![CDATA[
  	Finally, we have to divide both results by the evidence, or ‘P(X)’.
	]]>
    </string>
     
     <string name="patientcomputestatement6">
        Then, dividing the results by this value:
	</string>
    
      <string name="probabio">
	<![CDATA[
  	So, given the probabilities, does the patient have flu? To do this, we look at both probabilities \n 
  	and see which once has the highest value, and that is our answer. Since 0.701 is greater \n
  	than 0.227 then the answer is ‘no’, the patient does not have flu.
	]]>
    </string>
     
      <string name="comparisonanalysis">
	<![CDATA[
  	Comparing the two problems, why is it that the K Nearest Neighbor Classifier can be used on the Simpsons Scenario but not on the Patient Scenario? Why is Naive Bayes Classifier can be used in the Patient Scenario but not in the Simpson Scenario?
	\n\n
	Here is the complete analysis
	]]>
    </string>
      
      
    <string name="whydecisiontree">
	<![CDATA[
  	Why Decision Tree?
	Decision Tree works on data of any type. The DT algorithm is well-poised for analyzing very large databases  because it does not require loading all the data in machine main memory simultaneously. That is why, both problems we presented can be solved by Decision tree!
	Decision Tree algorithm helps solving the task of classifying cases into multiple categories. In many cases, this is the fastest, as well as easily interpreted machine learning algorithm. The DT algorithm provides intuitive rules for solving a great variety of classification tasks ranging from predicting buyers/non-buyers in database marketing, to automatically diagnosing patient in medicine, and to determining customer attrition causes in banking and insurance. No wonder the algorithm worked well on both our completely different scenarios – either with numerical or categorical data.
	\nCheck our reference on the web:\n
	http://www.bandmservices.com/DecisionTrees.htm

	]]>
    </string>   
    
    
       <string name="whynaive">
	<![CDATA[
  	Why Naive Bayes? \n 
	Naive Byes algorithms are among the most successful known algorithms for probabilistic classification. It predicts by reading a set of examples in attribute value-representation and then by using the Bayes Theorem to estimate the posterior probabilities of all qualifications. For each instance of the example, a classification with the highest posterier probability is chosen as the prediction. 

	In our Patient Scenario, Naive Bayes Classifier was a good choice because our data consist of flu symptoms described by their headache, runny nose, chills and fever. Bayesian Classifiers operate by saying, “If the patient has headache, runny nose, chills and fever, then the patient is most likely to have flu based on the observed data sample.”
	A difficulty arises when you have more than a few variables and classes with numerical data -- you would require an enormous number of observations (records) to estimate these probabilities.
	In other words, Naïve Bayes classifiers assume that the effect of an variable value on a given class is independent of the values of other variable. This assumption is called class conditional independence. It is made to simplify the computation and in this sense considered to be “Naïve”.
	This assumption is a fairly strong assumption and is often not applicable.  Naive bayes’s determine the classification based on the order of the probabilities, not their exact values. Naive bayes counts the frequency of the cases, so if the data contains unsimilar values, like in the Simpson Scenario, the classifier will not be able to perform accurately. 
	Check out these references: \n
	http://www.dmg.org/pmmlspecs_v2/NaiveBayes.html\n
	http://www.cs.iastate.edu/~patterbj/cs/cs572/algs/naive.html \n
	http://www.resample.com/xlminer/help/NaiveBC/classiNB_intro.htm \n
	]]>
    </string>
      
      
       
    <string name="whyknn">
	<![CDATA[
  	Why KNN \n
	The goal of K-Nearest Neighbor is to simply separate the data based on the assumed similarities between various classes.Thus, the classes can be differentiated from one another by searching for similarities between the data provided. In the Simpsons Scenario, we searched for the similarities of Comics Attribute to male or female attributes. 
	The K Nearest Neighbor is suitable for data with NUMERICAL values, like in our Simpsons Scenario. KNN finds the K neighbors nearest to the new samples from the training space based on some suitable similarity or distance metric.
	KNN is a good choice when simplicity and accuracy are the predominant issues. K-Nearest Neighbor is useful when there are less than 20 attributes per instance, there is lots of training data, training is very fast, learning complex target functions and dont want to loose information. Like in our Simpsons Scenario, there are only two numerical attributes per instance, which made KNN a suitable algorithm to use. 
	The disadvantage of using such function is that it does not handle categorical data effectively, such as in the Patient Scenario, because KNN does not rely on prior probabilities, but with distance. 
	 
	
	Check out these references on the web: \n
	http://cs.hbg.psu.edu/~ding/publications/PAKDD02_KNN.pdf \n
	http://wwwcsif.cs.ucdavis.edu/~liaoy/research/text_ss02_html/node4.html \n
	http://www.cs.tufts.edu/~emower/KNN.html \n
	http://www4.cs.umanitoba.ca/~jacky/Teaching/Courses/74.436/current/Lectures/L07_Instance_Based_Learning.pdf \n
	]]>
    </string>
      
      
    
 
</resources>